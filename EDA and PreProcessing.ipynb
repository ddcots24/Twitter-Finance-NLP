{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\somdd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('https')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('sent_train.csv')\n",
    "test = pd.read_csv('sent_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9538</th>\n",
       "      <td>The Week's Gainers and Losers on the Stoxx Eur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>Tupperware Brands among consumer gainers; Unil...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9540</th>\n",
       "      <td>vTv Therapeutics leads healthcare gainers; Myo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>WORK, XPO, PYX and AMKR among after hour movers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>YNDX, I, QD and OESX among tech movers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "9538  The Week's Gainers and Losers on the Stoxx Eur...      2\n",
       "9539  Tupperware Brands among consumer gainers; Unil...      2\n",
       "9540  vTv Therapeutics leads healthcare gainers; Myo...      2\n",
       "9541    WORK, XPO, PYX and AMKR among after hour movers      2\n",
       "9542             YNDX, I, QD and OESX among tech movers      2"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9543 entries, 0 to 9542\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    9543 non-null   object\n",
      " 1   label   9543 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 149.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.647386\n",
       "1    0.201509\n",
       "0    0.151106\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Took a look at the value counts to see if there is a  class imbalance\n",
    "train['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = [word_tokenize(doc) for doc in train['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146209,)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "flattenedcorpus_tokens = pd.Series(list(itertools.chain(*corpus)))\n",
    "print(flattenedcorpus_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26745\n"
     ]
    }
   ],
   "source": [
    "dictionary = pd.Series(\n",
    "    flattenedcorpus_tokens.unique())\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":                    7355\n",
       "https                5093\n",
       "$                    3310\n",
       ",                    2532\n",
       "to                   2338\n",
       "                     ... \n",
       "//t.co/uiXjEqyc6e       1\n",
       "Moynihan                1\n",
       "//t.co/ggKzSYVcPy       1\n",
       "//t.co/T2Zdt3jNiy       1\n",
       "zooplus                 1\n",
       "Length: 26745, dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to lowercase all tokenized words if they are capitals and not in the stop words\n",
    "def first_step_normalizer(doc):\n",
    "    norm_text = [x.lower() for x in word_tokenize(doc) if ((x.isalpha()) & (x.lower() not in stop_words))]\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(first_step_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14080\n"
     ]
    }
   ],
   "source": [
    "norm_toks_flattened = pd.Series(list(\n",
    "    itertools.chain(*train['text'])))\n",
    "new_dictionary = norm_toks_flattened.unique()\n",
    "print(len(new_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72991"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_toks_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock       911\n",
       "new         413\n",
       "results     373\n",
       "stocks      347\n",
       "earnings    347\n",
       "           ... \n",
       "basics        1\n",
       "orr           1\n",
       "gambler       1\n",
       "ntpc          1\n",
       "skyrizi       1\n",
       "Length: 14080, dtype: int64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_toks_flattened.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\somdd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\somdd\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(norm_toks_flattened))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "doc_lemmatized = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70474"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Normalized Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc):\n",
    "\n",
    "    #initialize lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "        \n",
    "    # remove stop words and punctuations, then lower case\n",
    "    doc_norm = [tok.lower() for tok in word_tokenize(doc) if ((tok.isalpha()) & (tok.lower() not in stop_words)) ]\n",
    "\n",
    "    #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "    \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(doc_norm))) \n",
    "    doc_norm = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "    \n",
    "    return doc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_corpus = train['text'].apply(process_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [bynd, jpmorgan, reel, expectation, meat]\n",
       "1       [ccl, rcl, nomura, point, booking, weakness, c...\n",
       "2       [cx, cemex, cut, credit, suisse, morgan, weak,...\n",
       "3                      [es, btig, research, cut, neutral]\n",
       "4                  [fnko, funko, slide, jaffray, pt, cut]\n",
       "                              ...                        \n",
       "9538    [week, gainer, loser, stoxx, europe, economy, ...\n",
       "9539    [tupperware, brand, consumer, gainer, unilever...\n",
       "9540    [vtv, therapeutic, lead, healthcare, gainer, m...\n",
       "9541                  [work, xpo, pyx, amkr, hour, mover]\n",
       "9542                        [yndx, qd, oesx, tech, mover]\n",
       "Name: text, Length: 9543, dtype: object"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11903"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_fully_norm = pd.Series(list(itertools.chain(*normalized_corpus)))\n",
    "len(flattened_fully_norm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock        1258\n",
       "market        537\n",
       "say           460\n",
       "report        419\n",
       "new           415\n",
       "             ... \n",
       "glabellar       1\n",
       "fury            1\n",
       "looks           1\n",
       "polarize        1\n",
       "slew            1\n",
       "Length: 11903, dtype: int64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_fully_norm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc = normalized_corpus.apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     bynd jpmorgan reel expectation meat\n",
       "1       ccl rcl nomura point booking weakness carnival...\n",
       "2       cx cemex cut credit suisse morgan weak buildin...\n",
       "3                            es btig research cut neutral\n",
       "4                         fnko funko slide jaffray pt cut\n",
       "                              ...                        \n",
       "9538    week gainer loser stoxx europe economy markets...\n",
       "9539    tupperware brand consumer gainer unilever lead...\n",
       "9540    vtv therapeutic lead healthcare gainer myomo b...\n",
       "9541                         work xpo pyx amkr hour mover\n",
       "9542                              yndx qd oesx tech mover\n",
       "Name: text, Length: 9543, dtype: object"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc.to_csv(\"finance_tweets_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     bynd jpmorgan reel expectation meat\n",
       "1       ccl rcl nomura point booking weakness carnival...\n",
       "2       cx cemex cut credit suisse morgan weak buildin...\n",
       "3                            es btig research cut neutral\n",
       "4                         fnko funko slide jaffray pt cut\n",
       "                              ...                        \n",
       "9538    week gainer loser stoxx europe economy markets...\n",
       "9539    tupperware brand consumer gainer unilever lead...\n",
       "9540    vtv therapeutic lead healthcare gainer myomo b...\n",
       "9541                         work xpo pyx amkr hour mover\n",
       "9542                              yndx qd oesx tech mover\n",
       "Name: text, Length: 9543, dtype: object"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
